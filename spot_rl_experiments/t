    def get_owlvit_det(self, arm_depth, save_image=False):
        marked_img = None
        if self.parallel_inference_mode:
            bbox_xy = str(self.detections_str_synced)
        else:
            # Get the image of the hand
            if USE_IMG_RESPONSE:
                # Get Spot camera image
                image_responses = self.spot.get_image_responses(IMG_SOURCES, quality=None)
                for image_response, source in zip(image_responses, IMG_SOURCES):
                    if source is SpotCamIds.HAND_COLOR:
                        img = image_response_to_cv2(image_response, reorient=True)
            else:
                bbox_xy = self.msg_to_cv2(self.msgs[rt.HAND_RGB])

            if save_image:
                marked_img = img.copy()

            self.owlvit.update_label([[self.owlvit_pick_up_object_name]])
            bbox_xy = self.owlvit.run_inference(img)

            self.curr_forget_steps = 0

        # Reset tue cur_forget_steps to avoid grasping fail
        self.curr_forget_steps = 0

        if bbox_xy is None or bbox_xy == 'None':
            return None

        if USE_IMG_RESPONSE:
            x1, y1, x2, y2 = bbox_xy
        else:
            x1, y1, x2, y2 = bbox_xy.split(',')
            x1 = int(x1)
            y1 = int(y1)
            x2 = int(x2)
            y2 = int(y2)

        # Create bbox mask from selected detection
        cx = int(np.mean([x1, x2]))
        cy = int(np.mean([y1, y2]))
        self.obj_center_pixel = (cx, cy)

        if save_image:
            if marked_img is None:
                while self.detection_timestamp not in self.detections_buffer['viz']:
                    pass
                viz_img = self.detections_buffer['viz'][self.detection_timestamp]
                marked_img = self.cv_bridge.imgmsg_to_cv2(viz_img)
                marked_img = cv2.resize(
                    marked_img,
                    (0, 0),
                    fx=1 / self.config.IMAGE_SCALE,
                    fy=1 / self.config.IMAGE_SCALE,
                    interpolation=cv2.INTER_AREA,
                )
            marked_img = cv2.circle(marked_img, (cx, cy), 5, (0, 0, 255), -1)
            marked_img = cv2.rectangle(marked_img, (x1, y1), (x2, y2), (0, 0, 255))
            out_path = osp.join(GRASP_VIS_DIR, f'{time.time()}.png')
            cv2.imwrite(out_path, marked_img)


        height, width = (480, 640)
        locked_on = self.locked_on_object(x1, y1, x2, y2, height, width)
        if locked_on:
            self.locked_on_object_count += 1
            print(f'Locked on to target {self.locked_on_object_count} time(s)...')
        else:
            if self.locked_on_object_count > 0:
                print('Lost lock-on!')
            self.locked_on_object_count = 0
        time.sleep(0.8)
        return x1, y1, x2, y2